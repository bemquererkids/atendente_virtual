# ğŸ“ app/agentes/agente_memoria.py

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from langchain.agents import Tool, initialize_agent
from app.memoria.historico_redis import obter_historico_usuario
from app.tools.especialidade_tool import responder_especialidade

# âš™ï¸ Inicializa o modelo da OpenAI com temperatura baixa para mais consistÃªncia nas respostas
llm = ChatOpenAI(model="gpt-4", temperature=0.3)

# ğŸ› ï¸ Lista de ferramentas disponÃ­veis para o agente (por enquanto, apenas especialidades)
ferramentas = [
    Tool(
        name="responder_especialidade",
        func=responder_especialidade,
        description=(
            "Usar quando a pessoa quiser saber se a clÃ­nica tem uma especialidade, o profissional que atende, "
            "e os diferenciais. Funciona com frase livre, desde que contenha algo como [clinica_id: bemquerer]."
        ),
    )
]

# ğŸ§  Template de prompt com histÃ³rico de conversa (mantÃ©m contexto entre mensagens)
prompt = ChatPromptTemplate.from_messages([
    ("system", "VocÃª Ã© uma secretÃ¡ria humanizada de uma clÃ­nica odontolÃ³gica, pronta para acolher e informar."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# ğŸ¤– ConfiguraÃ§Ã£o do agente com memÃ³ria de sessÃ£o via Redis
agente_com_memoria = RunnableWithMessageHistory(
    initialize_agent(
        tools=ferramentas,
        llm=llm,
        agent_type="chat-zero-shot-react-description",  # ğŸ§  Usa RAG zero-shot com descriÃ§Ã£o de tools
        verbose=True,
        handle_parsing_errors=True  # âš ï¸ Evita falha total caso o modelo retorne erro de parsing
    ),
    lambda session_id: obter_historico_usuario(session_id),
    input_messages_key="input",   # ğŸ”‘ Campo da mensagem de entrada
    history_messages_key="history"  # ğŸ” Campo do histÃ³rico da conversa
)